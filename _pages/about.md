---
layout: single
title: "Rongkun Zhou"
permalink: /
author_profile: true
---

<!-- ===== Self Summary ===== -->
**About Me**  
I'm Rongkun Zhou, a Master's student in Computer Science at Johns Hopkins University, broadly interested in natural language processing and machine learning, especially the reasoning capabilities and factual consistency of large language models.
My current focus is on improving the faithfulness, explainability, and robustness of NLP systems in tasks like question answering, retrieval, and reasoning over structured data.
I aim to develop models that generate not only fluent responses but also outputs grounded in knowledge sources and reliable in real-world scenarios.
---

## Education
- **Johns Hopkins University** â€” M.S. in Computer Science *(Expected Dec 2025)*  
- **University of Minnesota â€“ Twin Cities** â€” B.S. in Mathematics, Minor in Computer Science & Statistics *(Sept 2020=Dec 2023)*

---

## Ongoing Research <a id="ongoing"></a>
<em>More projects on the <a href="/research-experience/">Research Experience</a> page.</em>
### ğŸ§  Reasoning over Tabular Data  
*Research Assistant with Prof. Philipp Koehn, Johns Hopkins University â€” 2024.07â€“Present*  
Exploring how large language models perform structured reasoning over table data. We aim to develop and evaluate methods that improve the reliability, transparency, and factual grounding of model-generated reasoning traces in tabular question answering tasks.
### ğŸ® Debugging Reasoning Failures in Text-Based Games  
*Research Assistant with Prof. Ziang Xiao, Johns Hopkins University â€” 2024.08â€“Present*  
Applying the MindCoder framework to identify bottlenecks in agent reasoning during text-based game interaction. The goal is to detect stuck states where models fail to reason effectively and develop query-based strategies to guide model behavior more effectively in complex environments.
### ğŸ” Detecting Steganography in LLM Chain-of-Thought  
*Research Assistant with William Walden, HLTCOE, JHU â€” 2024.05â€“Present*  
Investigating how reasoning traces in LLM outputs might covertly encode information (textual steganography). This project aims to detect and mitigate hidden signals that may enable malicious coordination or model misuse, focusing on interventions aligned with knowledge distillation and output control.



