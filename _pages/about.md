---
layout: single
title: "Rongkun Zhou"
permalink: /
author_profile: true
---

<!-- ===== Self Summary ===== -->
I am Rongkun Zhou, a Masterâ€™s student in Computer Science at Johns Hopkins University.
My research interests lie in natural language processing and machine learning, with a focus on the reasoning capabilities of large language models. 
I aim to develop methods that improve structured reasoning, question answering, and information retrieval, ensuring that model outputs are not only fluent but also logically consistent, interpretable, and reliable in real-world applications.

## ğŸ“ Education
- **Johns Hopkins University** â€” M.S. in Computer Science *(Expected Dec 2025)*  
- **University of Minnesota â€“ Twin Cities** â€” B.S. in Mathematics, Minor in Computer Science & Statistics *(Sept 2020 â€“ Dec 2023)*

## ğŸ”¬ Ongoing Research <a id="ongoing"></a>
<em>More projects on the <a href="/research-project/">Research&Project</a> page.</em>

### ğŸ§  Reasoning over Tabular Data (2025.07â€“Present)  
- **Advisor**: Prof. Philipp Koehn, Johns Hopkins University  
- **Focus**: Exploring how large language models perform structured reasoning over table data  
- **Goal**: Improve the reliability and logical consistency of model-generated reasoning traces in tabular QA tasks  

### ğŸ® Debugging Reasoning Failures in Text-Based Games (2025.08â€“Present)  
- **Advisor**: Prof. Ziang Xiao, Johns Hopkins University  
- **Dataset/Framework**: TALES dataset, MindCoder  
- **Focus**: Investigating where language agents fail in multi-step reasoning and decision-making during interactive fiction scenarios  
- **Goal**: Identify reasoning bottlenecks and stuck states, guiding behavior through targeted interventions and improving agent robustness  

### ğŸ” Detecting Steganography in LLM Chain-of-Thought (2025.05â€“Present)  
- **Advisor**: William Walden, HLTCOE, Johns Hopkins University  
- **Focus**: Investigating how reasoning traces in LLM outputs might covertly encode information (textual steganography)  
- **Goal**: Detect and mitigate hidden signals that may enable malicious coordination or model misuse, using interventions such as knowledge distillation and controlled decoding to improve model safety, transparency, and trustworthiness  
