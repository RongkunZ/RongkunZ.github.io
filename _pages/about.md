---
layout: single
title: "Rongkun Zhou"
permalink: /
author_profile: true
---

<!-- ===== Self Summary ===== -->
I am Rongkun Zhou, a Master’s student in Computer Science at [Johns Hopkins University](https://www.jhu.edu/).
My research interests lie in natural language processing and machine learning, with a focus on the reasoning capabilities of large language models. 
I aim to develop methods that improve structured reasoning, question answering, and information retrieval, ensuring that model outputs are not only fluent but also logically consistent, interpretable, and reliable in real-world applications.

## 🎓 Education
- **[Johns Hopkins University](https://www.jhu.edu/)** — M.S. in Computer Science *(Expected Dec 2025)*  
- **[University of Minnesota – Twin Cities](https://twin-cities.umn.edu/)** — B.S. in Mathematics, Minor in Computer Science & Statistics *(Sept 2020 – Dec 2023)*

## 🔬 Ongoing Research <a id="ongoing"></a>
<em>More projects on the <a href="/research-project/">Research&Project</a> page.</em>

### 🧠 Reasoning over Tabular Data (2025.07–Present)  
- **Advisor**: Prof. Philipp Koehn, Johns Hopkins University  
- **Focus**: Exploring how large language models perform structured reasoning over table data  
- **Goal**: Improve the reliability and logical consistency of model-generated reasoning traces in tabular QA tasks  

### 🎮 Debugging Reasoning in Text Games (2025.08–Present)  
- **Advisor**: Prof. Ziang Xiao, Johns Hopkins University  
- **Dataset/Framework**: [TALES dataset](https://arxiv.org/abs/2504.14128), [MindCoder](https://arxiv.org/abs/2501.00775)  
- **Focus**: Investigating where language agents fail in multi-step reasoning and decision-making during interactive fiction scenarios  
- **Goal**: Identify reasoning bottlenecks and stuck states, guiding behavior through targeted interventions and improving agent robustness  

### 🔐 Steganography in LLM Chain-of-Thought (2025.05–Present)  
- **Advisor**: William Walden, [HLTCOE](https://hltcoe.jhu.edu/), Johns Hopkins University  
- **Focus**: Investigating how reasoning traces in LLM outputs might covertly encode information (textual steganography)  
- **Goal**: Detect and mitigate hidden signals that may enable malicious coordination or model misuse, using interventions such as knowledge distillation and controlled decoding to improve model safety, transparency, and trustworthiness  
