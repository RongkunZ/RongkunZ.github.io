---
layout: single
title: "Rongkun Zhou"
permalink: /
author_profile: true
---

<!-- ===== Self Summary ===== -->
I am Rongkun Zhou, a Master’s student in Computer Science at Johns Hopkins University.
My research interests lie in natural language processing and machine learning, with a focus on the reasoning capabilities of large language models. 
I aim to develop methods that improve structured reasoning, question answering, and information retrieval, ensuring that model outputs are not only fluent but also logically consistent, interpretable, and reliable in real-world applications.

## 🎓 Education
- **Johns Hopkins University** — M.S. in Computer Science *(Expected Dec 2025)*  
- **University of Minnesota – Twin Cities** — B.S. in Mathematics, Minor in Computer Science & Statistics *(Sept 2020 – Dec 2023)*

## 🔬 Ongoing Research <a id="ongoing"></a>
<em>More projects on the <a href="/research-project/">Research&Project</a> page.</em>

### 🧠 Reasoning over Tabular Data  
*Research Assistant with Prof. Philipp Koehn, Johns Hopkins University — 2025.07–Present*
Exploring how large language models perform structured reasoning over table data.  
We aim to improve the reliability and logical consistency of model-generated reasoning traces in tabular QA tasks.

### 🎮 Debugging Reasoning Failures in Text-Based Games  
*Research Assistant with Prof. Ziang Xiao, Johns Hopkins University — 2025.08–Present*
Working on the TALES dataset and using the MindCoder framework to investigate where language agents fail in multi-step reasoning and decision-making during interactive fiction scenarios.  
This project identifies reasoning bottlenecks and stuck states by analyzing model trajectories, with the goal of guiding behavior through targeted interventions and improving agent robustness.

### 🔐 Detecting Steganography in LLM Chain-of-Thought  
*Research Assistant with William Walden, HLTCOE, JHU — 2025.05–Present*  
Investigating how reasoning traces in LLM outputs might covertly encode information (textual steganography).  
This project aims to detect and mitigate hidden signals that may enable malicious coordination or model misuse, focusing on interventions such as knowledge distillation and controlled decoding to improve model safety, transparency, and trustworthiness.
